# ğŸ¤– HR Policies Assistant - RAG Chatbot

An intelligent **Retrieval-Augmented Generation (RAG) chatbot** that answers questions from HR policy documents using **PDF ingestion, vector embeddings, Pinecone vector database, and Llama 2 (Ollama)**. The chatbot supports both **API-based querying** and a **Streamlit frontend**.

---

## ğŸ“„ Features

- âœ… Ingest multiple PDF HR policy documents automatically.
- âœ… Split documents into chunks for efficient embedding and retrieval.
- âœ… Generate semantic embeddings with **Sentence Transformers**.
- âœ… Store and query document embeddings using **Pinecone vector database**.
- âœ… Retrieve relevant context with a **similarity search**.
- âœ… Generate concise answers with **Llama 2 (Ollama)** using retrieved context.
- âœ… Fallback to LLM for queries without relevant documents.
- âœ… REST API using **FastAPI**.
- âœ… Interactive frontend with **Streamlit**, including:
  - Connection status
  - RAG parameter tuning (Top K, similarity threshold)
  - Clear chat history
  - Real-time chat interface

---

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|------------|
| Backend | Python, FastAPI |
| Frontend | Streamlit |
| Vector Store | Pinecone |
| Embeddings | Sentence Transformers (`all-MiniLM-L6-v2`) |
| LLM | Llama 2 via Ollama |
| Document Loading | PyMuPDF |
| Document Splitting | RecursiveCharacterTextSplitter (LangChain) |
| API & Env | Python `dotenv`, Pydantic |

---

## ğŸ—ï¸ Project Structure

hr-policies-assistant/
â”œâ”€â”€ backend.py # FastAPI backend with RAG & LLM integration
â”œâ”€â”€ streamlit_app.py # Streamlit frontend for chat interface
â”œâ”€â”€ embeddings.py # EmbeddingManager for text/PDFs
â”œâ”€â”€ vectorstore.py # Pinecone integration & similarity search
â”œâ”€â”€ rag_retriever.py # RAGRetriever class
â”œâ”€â”€ data/policies/ # HR PDF/text documents
â”œâ”€â”€ .env # API keys & configuration
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md


---

## âš¡ Setup Instructions

### 1. Clone Repository
```bash
git clone https://github.com/Shah-Abdul-Mazid/HR-Policies-Assistant.git
cd HR-Policies-Assistant


2. Create Virtual Environment
python -m venv .venv
# Activate
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

3. Install Dependencies
pip install -r requirements.txt

4. Configure Environment Variables

Create a .env file:

API_KEY=<your_pinecone_api_key>
OLLAMA_API_KEY=<your_ollama_key_if_needed>

5. Add HR Documents

Place PDF files in:

data/policies/

6. Run Backend
python backend.py


FastAPI will run at:

http://localhost:8000


Swagger UI (interactive API docs):

http://localhost:8000/docs

7. Run Frontend
streamlit run streamlit_app.py

ğŸ’¬ API Usage
POST /ask

Ask a question to the RAG chatbot.

Request Body:

{
  "query": "What is the company's leave policy?",
  "top_k": 5,
  "score_threshold": 0.4,
  "fallback_to_llm": true
}


Response:

{
  "query": "What is the company's leave policy?",
  "answer": "Employees are entitled to 20 days of paid leave per year...",
  "status": "success"
}

ğŸ§  How It Works

PDF Ingestion: Load HR PDFs using PyMuPDFLoader.

Document Chunking: Split PDFs into smaller chunks using RecursiveCharacterTextSplitter.

Embeddings: Generate semantic embeddings using Sentence Transformers.

Vector Storage: Store embeddings in Pinecone with metadata.

Retrieval: Perform similarity search on query embeddings.

Generation: Use Llama 2 (Ollama) to generate answers using retrieved context.

Fallback: If no relevant context is found, optionally use LLM alone.

ğŸ¨ Streamlit Frontend

Real-time chat interface with user and assistant messages.

Display message timestamps.

Adjustable Top K and similarity threshold.

LLM fallback toggle.

Clear chat history and connection status indicators.

Custom CSS styling for modern UI.

ğŸ“ˆ Customization

Change embedding model: all-MiniLM-L6-v2 â†’ any HuggingFace transformer.

Adjust chunk_size and chunk_overlap in split_documents() for more/less context.

Tune top_k and score_threshold in API or frontend for better precision.